{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "### Cross Validation and Model Selection metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss \n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import jaccard_similarity_score as jaccard_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Preprocessing\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler as Standardize\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "### Classification\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForest\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as Log_Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data shape: (5996L, 124L)\n",
      "X_data_std shape: (5996L, 124L)\n",
      "y shape (5996L, 20L)\n"
     ]
    }
   ],
   "source": [
    "### Load Dataset\n",
    "# X: Unprocessed features\n",
    "# X_std: standardized by Preprocessor\n",
    "# y: MultiLabel Binarized targets\n",
    "[X_data, X_data_std, y_data] = pickle.load(open('data/cleaned_data_for_traditional_models.p', 'r'))\n",
    "\n",
    "print 'X_data shape:', X_data.shape\n",
    "print 'X_data_std shape:', X_data_std.shape\n",
    "print 'y shape', y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting traditional models on the data\n",
    "\n",
    "The following traditional models were fitted on the data using **One-vs-Rest classifier** for all the genres. The relevant parameters were tuned using 3-fold cross validation. \n",
    "\n",
    " - Weighted Logistic regression: Tuned **Regularization Parameter C** using Cross-Validation.  \n",
    " - Decision Tree: Tuned **max_depth** using cross validation\n",
    " - Random Forest: Tuned **max_depth** using cross validation\n",
    " - AdaBoost: Tuned **n_estimators** using cross validation\n",
    " \n",
    "** The models were run on separate computers. The results were saved in a pickle file for each and were analyzed below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tune models by minimizing Hamming Loss\n",
    "hamming_scorer = make_scorer(hamming_loss, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Weighted Logistic Regression\n",
    "# use L2 regularization, and balanced weight\n",
    "LogReg_Model = OneVsRestClassifier(Log_Reg(penalty = 'l2', class_weight = 'balanced'))\n",
    "\n",
    "# grid search for regularization parameter C\n",
    "LogReg_grid = GridSearchCV(LogReg_Model, \n",
    "                           param_grid={'estimator__C': np.logspace(-5, 15, 20)}, \n",
    "                                       scoring= hamming_scorer,\n",
    "                                       n_jobs = 5)\n",
    "LogReg_grid.fit(X_data_std, y_data)\n",
    "\n",
    "# Fit the best model on testing data\n",
    "y_pred_LogReg = cross_val_predict(LogReg_grid.best_estimator_, X_data_std, y_data)\n",
    "\n",
    "# Dump CV results and predictions from best model\n",
    "pickle.dump([LogReg_grid.cv_results_, y_pred_LogReg], open('results/traditional/LogReg_grid_results.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Single Decision Tree\n",
    "DecisionTree_Model = OneVsRestClassifier(tree.DecisionTreeClassifier(criterion='gini'))\n",
    "# grid search on max_depth\n",
    "DT_grid = GridSearchCV(DecisionTree_Model, \n",
    "                    param_grid = {'estimator__max_depth': range(1,10)},\n",
    "                                  scoring = hamming_scorer)\n",
    "DT_grid.fit(X_data_std, y_data)\n",
    "\n",
    "# Fit the best model on testing data\n",
    "y_pred_Decision_Tree = cross_val_predict(DT_grid.best_estimator_, X_data_std, y_data)\n",
    "\n",
    "# Dump CV results and predictions from best model\n",
    "pickle.dump([DT_grid.cv_results_, y_pred_Decision_Tree], open('results/traditional/DecisionTree_grid_results.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Random Forest\n",
    "RandomForest_Model = OneVsRestClassifier(RandomForest())\n",
    "# grid search on max_depth\n",
    "rf_grid = GridSearchCV(RandomForest_Model,\n",
    "                       param_grid = {'estimator__max_depth': 10*np.linspace(1,7, 7) },\n",
    "                       scoring = hamming_scorer)\n",
    "\n",
    "rf_grid.fit(X_data_std, y_data)\n",
    "\n",
    "# Fit the best model on testing data\n",
    "y_pred_RF = cross_val_predict(rf_grid.best_estimator_, X_data_std, y_data)\n",
    "\n",
    "# Dump CV results and predictions from best model\n",
    "pickle.dump([rf_grid.cv_results_, y_pred_RF], open('results/traditional/RandomForest_grid_results.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Adaboost\n",
    "Ada = OneVsRestClassifier(AdaBoostClassifier())\n",
    "# grid search on the number of estimators\n",
    "ada_grid = GridSearchCV(Ada,\n",
    "                       param_grid = {'estimator__n_estimators': np.logspace(1,3,6).astype(int) },\n",
    "                                     scoring = hamming_scorer)\n",
    "ada_grid.fit(X_data_std, y_data)\n",
    "\n",
    "# Dump CV results and predictions from best model\n",
    "y_pred_ada = cross_val_predict(ada_grid.best_estimator_, X_data_std, y_data)\n",
    "pickle.dump([ada_grid.cv_results_, y_pred_ada], open('results/traditional/Adaboost_grid_results.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Model Performance for Multilabel Classification\n",
    "\n",
    "To perform multilabel classification using the OneVsAll classifier, we used several different metrics to assess model performance. Six different metrics were used as the performance metrics of our model: 0-1 loss based accuracy, Hamming loss, precision, recall, F1-score, and Jaccard Similarity. In addition to reporting these metrics for the overall dataset under different models, we evaluated each of these metrics for each genre label, to detect whether any labels were being excluded from prediction due to imbalance. \n",
    "\n",
    "\n",
    "#### 1. Accuracy\n",
    "\n",
    "Accuracy is calculated on a zero-one loss basis, which counts a match between the prediction and true value as one. It then takes the mean of all the indicator and produced the percentage that the binary entries in prediction matrix correctly match up with true values.\n",
    "\n",
    "#### 2. Hamming Loss\n",
    "\n",
    "Simply put, the Hamming loss is the fraction of labels that are incorrectly predicted, penalized according to sample weights. In this case, Hamming loss is $1 - accuracy$  \n",
    "\n",
    "#### 3. Precision\n",
    "\n",
    "$$Precision=\\frac{tp}{tp+fp}$$\n",
    "where tp = True Positives and fp = False Positives\n",
    "\n",
    "#### 4. Recall\n",
    "\n",
    "$$Recall=\\frac{tp}{tp+fn}$$ \n",
    "where tp = True Positives and fn = False Negatives\n",
    "\n",
    "#### 5. F-1 Score\n",
    "\n",
    "F-1 score is the harmonic mean of Precision and Recall. It is a measurement that considers both Precision and Recall. \n",
    "\n",
    "$$F_{1}=2\\cdot {\\frac {1}{{\\tfrac {1}{\\mathrm {recall} }}+{\\tfrac {1}{\\mathrm {precision} }}}}=2\\cdot {\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{\\mathrm {precision} +\\mathrm {recall} }}$$\n",
    "\n",
    "#### 6. Jaccard Similarities\n",
    "\n",
    "The Jaccard Similarity between the predicted labels (y_pred) and ground truth labels (y_data) is defined as the intersection divided by the size of the union of the two label sets for a given data point $X_i$. Therefore, JS penalizes the inclusion or exclusion of true labels from the prediction. \n",
    "\n",
    "$$ J(A,B) = {{|A \\cap B|}\\over{|A \\cup B|}} = {{|A \\cap B|}\\over{|A| + |B| - |A \\cap B|}}$$\n",
    "where A = dataset labels, B = multilabel classification output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Unpickle Model Results\n",
    "[LogReg_cv, y_pred_LogReg] = pickle.load(open('results/traditional/LogReg_grid_results.p', 'r'))\n",
    "\n",
    "[DT_grid_cv, y_pred_Decision_Tree] = pickle.load(open('results/traditional/DecisionTree_grid_results.p', 'r'))\n",
    "\n",
    "[rf_grid_cv, y_pred_RF] = pickle.load(open('results/traditional/RandomForest_grid_results.p', 'r'))\n",
    "\n",
    "[ada_grid_cv, y_pred_ada] = pickle.load(open('results/traditional/Adaboost_grid_results.p', 'r'))\n",
    "\n",
    "# Create list with all model prediction\n",
    "prediction_list = [y_pred_LogReg, y_pred_Decision_Tree, y_pred_RF, y_pred_ada]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read column names to get genre labels for tables below\n",
    "genre_ids = pickle.load(open('data/binarizer_genre_list.p', 'r'))\n",
    "\n",
    "# read the genre list (genre ids and names)\n",
    "genre_list = pd.read_csv('data/genre_list.csv')\n",
    "\n",
    "# add foreign movies (a genre that is not included in the downloaded genre list)\n",
    "foreign = pd.DataFrame({'id': [10769], 'GenreName': ['Foreign']})\n",
    "genre_list = pd.concat([genre_list, foreign], axis = 0)\n",
    "\n",
    "# order the genrenames according to the binarizer classes\n",
    "genre_labels = []\n",
    "for genre_id in genre_ids:\n",
    "    genre = genre_list['GenreName'][genre_list['id'] == genre_id].values[0]\n",
    "    genre_labels.append(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adventure',\n",
       " 'Fantasy',\n",
       " 'Animation',\n",
       " 'Drama',\n",
       " 'Horror',\n",
       " 'Action',\n",
       " 'Comedy',\n",
       " 'History',\n",
       " 'Western',\n",
       " 'Thriller',\n",
       " 'Crime',\n",
       " 'Documentary',\n",
       " 'Science Fiction',\n",
       " 'Mystery',\n",
       " 'Music',\n",
       " 'Romance',\n",
       " 'Family',\n",
       " 'War',\n",
       " 'Foreign',\n",
       " 'TV Movie']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the order of the genres\n",
    "genre_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### SUMMARIZE MODEL ACCURACY: \n",
    "    # for MULTILABEL DATA, calculates baseline accuracy, hamming loss, f1 score, jaccard similarity, classification report\n",
    "    # INPUTS:\n",
    "        # y_prediction: predicted y\n",
    "        # y_data : ground truth y\n",
    "    # OUTPUTS:\n",
    "        # prints accuracy metrics\n",
    "        # Return 0\n",
    "\n",
    "def summarize_model_accuracy (y_prediction, y_data, names):\n",
    "    # Get basic accuracy: what proportion of labels are correct\n",
    "    print 'Accuracy:', np.mean(y_prediction == y_data)\n",
    "    \n",
    "    # Get Hamming Loss\n",
    "    print 'Hamming Loss:', hamming_loss(y_data, y_prediction)\n",
    "    \n",
    "    # Get f1\n",
    "    print 'F1 Score:', f1_score(y_data, y_prediction, average = 'weighted')\n",
    "    \n",
    "    # get Jaccard Similarity\n",
    "    print 'Jaccard Similarity:', jaccard_score(y_data, y_prediction)\n",
    "    \n",
    "    # Classification report:report recall, precision, f1 ON EACH CLASS (can be used for multilabel case)\n",
    "    print classification_report(y_data, y_prediction, target_names = names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.595488659106\n",
      "Hamming Loss: 0.404511340894\n",
      "F1 Score: 0.363745775457\n",
      "Jaccard Similarity: 0.151680275982\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.10      0.61      0.18       367\n",
      "        Fantasy       0.07      0.66      0.13       268\n",
      "      Animation       0.10      0.80      0.18       339\n",
      "          Drama       0.52      0.67      0.58      2179\n",
      "         Horror       0.22      0.74      0.35       856\n",
      "         Action       0.20      0.74      0.32       774\n",
      "         Comedy       0.36      0.68      0.47      1496\n",
      "        History       0.04      0.80      0.07       125\n",
      "        Western       0.02      0.73      0.03        55\n",
      "       Thriller       0.29      0.76      0.42      1157\n",
      "          Crime       0.09      0.71      0.17       396\n",
      "    Documentary       0.30      0.91      0.45       909\n",
      "Science Fiction       0.11      0.77      0.20       422\n",
      "        Mystery       0.06      0.67      0.11       269\n",
      "          Music       0.08      0.73      0.15       286\n",
      "        Romance       0.20      0.67      0.30       650\n",
      "         Family       0.15      0.69      0.24       460\n",
      "            War       0.02      0.79      0.04        73\n",
      "        Foreign       0.01      0.47      0.02        74\n",
      "       TV Movie       0.07      0.64      0.12       227\n",
      "\n",
      "    avg / total       0.27      0.72      0.36     11382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summarize LogReg Performance\n",
    "LogRegSummary = summarize_model_accuracy(y_pred_LogReg, y_data, genre_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.908522348232\n",
      "Hamming Loss: 0.0914776517678\n",
      "F1 Score: 0.168331087028\n",
      "Jaccard Similarity: 0.13245377871\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.59      0.04      0.08       367\n",
      "        Fantasy       0.14      0.01      0.01       268\n",
      "      Animation       0.45      0.05      0.09       339\n",
      "          Drama       0.56      0.27      0.37      2179\n",
      "         Horror       0.00      0.00      0.00       856\n",
      "         Action       0.49      0.03      0.06       774\n",
      "         Comedy       0.86      0.12      0.22      1496\n",
      "        History       0.08      0.01      0.01       125\n",
      "        Western       0.00      0.00      0.00        55\n",
      "       Thriller       0.25      0.00      0.00      1157\n",
      "          Crime       0.00      0.00      0.00       396\n",
      "    Documentary       0.82      0.30      0.44       909\n",
      "Science Fiction       0.30      0.03      0.06       422\n",
      "        Mystery       0.00      0.00      0.00       269\n",
      "          Music       0.65      0.15      0.24       286\n",
      "        Romance       0.42      0.11      0.18       650\n",
      "         Family       0.61      0.04      0.08       460\n",
      "            War       0.29      0.05      0.09        73\n",
      "        Foreign       0.00      0.00      0.00        74\n",
      "       TV Movie       0.34      0.07      0.12       227\n",
      "\n",
      "    avg / total       0.47      0.11      0.17     11382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Decision_Tree = summarize_model_accuracy(y_pred_Decision_Tree, y_data, genre_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.909372915277\n",
      "Hamming Loss: 0.0906270847231\n",
      "F1 Score: 0.215170777762\n",
      "Jaccard Similarity: 0.169916055148\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.52      0.06      0.11       367\n",
      "        Fantasy       0.27      0.01      0.02       268\n",
      "      Animation       0.52      0.04      0.07       339\n",
      "          Drama       0.60      0.34      0.43      2179\n",
      "         Horror       0.62      0.09      0.16       856\n",
      "         Action       0.44      0.05      0.09       774\n",
      "         Comedy       0.64      0.17      0.27      1496\n",
      "        History       0.00      0.00      0.00       125\n",
      "        Western       0.00      0.00      0.00        55\n",
      "       Thriller       0.40      0.10      0.16      1157\n",
      "          Crime       0.14      0.00      0.00       396\n",
      "    Documentary       0.76      0.35      0.48       909\n",
      "Science Fiction       0.47      0.02      0.04       422\n",
      "        Mystery       0.00      0.00      0.00       269\n",
      "          Music       0.63      0.08      0.15       286\n",
      "        Romance       0.48      0.07      0.12       650\n",
      "         Family       0.52      0.06      0.11       460\n",
      "            War       0.00      0.00      0.00        73\n",
      "        Foreign       0.00      0.00      0.00        74\n",
      "       TV Movie       0.33      0.03      0.06       227\n",
      "\n",
      "    avg / total       0.51      0.15      0.22     11382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RF_Model_Summary = summarize_model_accuracy(y_pred_RF, y_data, genre_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.909856571047\n",
      "Hamming Loss: 0.0901434289526\n",
      "F1 Score: 0.265702827649\n",
      "Jaccard Similarity: 0.21204890562\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.49      0.10      0.17       367\n",
      "        Fantasy       0.00      0.00      0.00       268\n",
      "      Animation       0.43      0.09      0.15       339\n",
      "          Drama       0.58      0.39      0.47      2179\n",
      "         Horror       0.62      0.19      0.29       856\n",
      "         Action       0.47      0.09      0.15       774\n",
      "         Comedy       0.70      0.18      0.29      1496\n",
      "        History       0.26      0.05      0.08       125\n",
      "        Western       0.00      0.00      0.00        55\n",
      "       Thriller       0.46      0.15      0.23      1157\n",
      "          Crime       0.00      0.00      0.00       396\n",
      "    Documentary       0.71      0.45      0.55       909\n",
      "Science Fiction       0.43      0.02      0.04       422\n",
      "        Mystery       0.00      0.00      0.00       269\n",
      "          Music       0.62      0.13      0.22       286\n",
      "        Romance       0.52      0.13      0.21       650\n",
      "         Family       0.40      0.08      0.13       460\n",
      "            War       0.20      0.04      0.07        73\n",
      "        Foreign       0.00      0.00      0.00        74\n",
      "       TV Movie       0.45      0.11      0.18       227\n",
      "\n",
      "    avg / total       0.50      0.19      0.27     11382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ada_Boost = summarize_model_accuracy(y_pred_ada, y_data, genre_labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
